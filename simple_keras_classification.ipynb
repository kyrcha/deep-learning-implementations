{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple-keras-classification",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyrcha/deep-learning-pipelines/blob/master/simple_keras_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6qSEbGKnG9B",
        "colab_type": "text"
      },
      "source": [
        "# Simple Keras model for classification\n",
        "\n",
        "For a dataset in this simple Keras classification example we will use the [Wisconsin breast cancer dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29). We will upload data from the local drive and in particular the file `breast-cancer-wisconsin.data`. Other ways to upload files to a Google colab notebook can be found [here](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHpx6L5SmISJ",
        "colab_type": "code",
        "outputId": "943fffd4-a4e9-45ff-b204-1bc462407ed9",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7b9e881e-52e8-4a3d-ace0-95d8fda2fe23\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7b9e881e-52e8-4a3d-ace0-95d8fda2fe23\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving breast-cancer-wisconsin.data to breast-cancer-wisconsin.data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxHcVAfJ8m8j",
        "colab_type": "text"
      },
      "source": [
        "Now let's read the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwenudK-onFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "data = pd.read_csv(io.BytesIO(uploaded['breast-cancer-wisconsin.data']), \n",
        "                   header=None, na_values='?')\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2O6JCU9o9RX",
        "colab_type": "text"
      },
      "source": [
        "Let's check if it was uploaded and parsed correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY3aBpoyo4-d",
        "colab_type": "code",
        "outputId": "702ad501-2dcb-4dc1-a67e-517ea3ea253c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000025</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1002945</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1015425</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1016277</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1017023</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        0   1   2   3   4   5     6   7   8   9   10\n",
              "0  1000025   5   1   1   1   2   1.0   3   1   1   2\n",
              "1  1002945   5   4   4   5   7  10.0   3   2   1   2\n",
              "2  1015425   3   1   1   1   2   2.0   3   1   1   2\n",
              "3  1016277   6   8   8   1   3   4.0   3   7   1   2\n",
              "4  1017023   4   1   1   3   2   1.0   3   1   1   2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFb0mBxip6l5",
        "colab_type": "text"
      },
      "source": [
        "We will drop the first column which is the id of the sample and will not provide anything to the predictive ability of the model and also drop samples with missing values in order to have a pristine dataset. Then we will print the summary statistics of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM24NlLSpBhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.drop(0, axis=1).dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sOWKP4juhoQ",
        "colab_type": "code",
        "outputId": "d5eea455-0dd3-4b0c-8184-afd9ac49711d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4.442167</td>\n",
              "      <td>3.150805</td>\n",
              "      <td>3.215227</td>\n",
              "      <td>2.830161</td>\n",
              "      <td>3.234261</td>\n",
              "      <td>3.544656</td>\n",
              "      <td>3.445095</td>\n",
              "      <td>2.869693</td>\n",
              "      <td>1.603221</td>\n",
              "      <td>2.699854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.820761</td>\n",
              "      <td>3.065145</td>\n",
              "      <td>2.988581</td>\n",
              "      <td>2.864562</td>\n",
              "      <td>2.223085</td>\n",
              "      <td>3.643857</td>\n",
              "      <td>2.449697</td>\n",
              "      <td>3.052666</td>\n",
              "      <td>1.732674</td>\n",
              "      <td>0.954592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               1           2           3   ...          8           9           10\n",
              "count  683.000000  683.000000  683.000000  ...  683.000000  683.000000  683.000000\n",
              "mean     4.442167    3.150805    3.215227  ...    2.869693    1.603221    2.699854\n",
              "std      2.820761    3.065145    2.988581  ...    3.052666    1.732674    0.954592\n",
              "min      1.000000    1.000000    1.000000  ...    1.000000    1.000000    2.000000\n",
              "25%      2.000000    1.000000    1.000000  ...    1.000000    1.000000    2.000000\n",
              "50%      4.000000    1.000000    1.000000  ...    1.000000    1.000000    2.000000\n",
              "75%      6.000000    5.000000    5.000000  ...    4.000000    1.000000    4.000000\n",
              "max     10.000000   10.000000   10.000000  ...   10.000000   10.000000    4.000000\n",
              "\n",
              "[8 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgAuDQljuzAo",
        "colab_type": "text"
      },
      "source": [
        "We will normalize the data between 0 and 1 by dividing the attributes by 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjd2jE5Fpwd9",
        "colab_type": "code",
        "outputId": "ba591d40-c7cd-4719-e519-28ef58e5313b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "\n",
        "data.iloc[:,0:9] = data.iloc[:,0:9].div(10)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.6</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    1    2    3    4    5    6    7    8    9   10\n",
              "0  0.5  0.1  0.1  0.1  0.2  0.1  0.3  0.1  0.1   2\n",
              "1  0.5  0.4  0.4  0.5  0.7  1.0  0.3  0.2  0.1   2\n",
              "2  0.3  0.1  0.1  0.1  0.2  0.2  0.3  0.1  0.1   2\n",
              "3  0.6  0.8  0.8  0.1  0.3  0.4  0.3  0.7  0.1   2\n",
              "4  0.4  0.1  0.1  0.3  0.2  0.1  0.3  0.1  0.1   2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7H-KONexcib",
        "colab_type": "text"
      },
      "source": [
        "Now let's try to predict whether the cancer is benign (2) or malignant (4), based on the 9 available features. First we will do a Train-Test split of 80-20%. We will keep the 20% to test the model in the end. We will also do one-hot-encoding for the class variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84dW0QX8zxJc",
        "colab_type": "code",
        "outputId": "37331c0f-3889-4127-a23b-8d09e909407d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "data = shuffle(data)\n",
        "\n",
        "X = data.iloc[:,0:8]\n",
        "y = data.iloc[:,9].apply(str)\n",
        "y[y == '2'] = '0'\n",
        "y[y == '4'] = '1'\n",
        "from keras.utils import to_categorical\n",
        "encoded_y = to_categorical(y)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, encoded_y, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=1234)\n",
        "\n",
        "print(X_train[:5])\n",
        "print(y_train[:5])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       1    2    3    4    5    6    7    8\n",
            "344  0.7  0.6  0.4  0.8  1.0  1.0  0.9  0.5\n",
            "22   0.3  0.1  0.1  0.1  0.2  0.1  0.2  0.1\n",
            "124  0.5  0.4  0.6  0.7  0.9  0.7  0.8  1.0\n",
            "239  1.0  0.4  0.3  0.2  0.3  1.0  0.5  0.3\n",
            "552  0.3  0.2  0.2  0.2  0.2  0.1  0.4  0.2\n",
            "[[0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCoAInBzstEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6LPpu7d937z",
        "colab_type": "text"
      },
      "source": [
        "We will build a simple feedforward neural net with one hidden layer with 20 nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsiQr5Jkx1UQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l0 = tf.keras.layers.Dense(units=20, input_shape=(8,), \n",
        "                           activation='relu')\n",
        "l1 = tf.keras.layers.Dense(units=2, activation='sigmoid')  \n",
        "model = tf.keras.Sequential([l0, l1])\n",
        "model.compile(optimizer ='adam', loss='binary_crossentropy', \n",
        "              metrics =['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWwQdkmU3Zy-",
        "colab_type": "code",
        "outputId": "493c39e3-c911-40b9-c2d3-d7459b47e114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=100, validation_split=0.1)\n",
        "print('Finished training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 491 samples, validate on 55 samples\n",
            "Epoch 1/100\n",
            "491/491 [==============================] - 0s 419us/sample - loss: 0.6856 - acc: 0.4919 - val_loss: 0.6642 - val_acc: 0.5091\n",
            "Epoch 2/100\n",
            "491/491 [==============================] - 0s 55us/sample - loss: 0.6624 - acc: 0.5662 - val_loss: 0.6394 - val_acc: 0.6909\n",
            "Epoch 3/100\n",
            "491/491 [==============================] - 0s 55us/sample - loss: 0.6394 - acc: 0.7566 - val_loss: 0.6147 - val_acc: 0.8455\n",
            "Epoch 4/100\n",
            "491/491 [==============================] - 0s 53us/sample - loss: 0.6150 - acc: 0.8595 - val_loss: 0.5874 - val_acc: 0.9000\n",
            "Epoch 5/100\n",
            "491/491 [==============================] - 0s 56us/sample - loss: 0.5867 - acc: 0.9053 - val_loss: 0.5581 - val_acc: 0.9000\n",
            "Epoch 6/100\n",
            "491/491 [==============================] - 0s 56us/sample - loss: 0.5573 - acc: 0.9236 - val_loss: 0.5270 - val_acc: 0.9091\n",
            "Epoch 7/100\n",
            "491/491 [==============================] - 0s 55us/sample - loss: 0.5262 - acc: 0.9318 - val_loss: 0.4961 - val_acc: 0.9182\n",
            "Epoch 8/100\n",
            "491/491 [==============================] - 0s 57us/sample - loss: 0.4940 - acc: 0.9409 - val_loss: 0.4651 - val_acc: 0.9273\n",
            "Epoch 9/100\n",
            "491/491 [==============================] - 0s 51us/sample - loss: 0.4611 - acc: 0.9481 - val_loss: 0.4343 - val_acc: 0.9273\n",
            "Epoch 10/100\n",
            "491/491 [==============================] - 0s 59us/sample - loss: 0.4279 - acc: 0.9521 - val_loss: 0.4033 - val_acc: 0.9273\n",
            "Epoch 11/100\n",
            "491/491 [==============================] - 0s 55us/sample - loss: 0.3941 - acc: 0.9552 - val_loss: 0.3731 - val_acc: 0.9273\n",
            "Epoch 12/100\n",
            "491/491 [==============================] - 0s 58us/sample - loss: 0.3610 - acc: 0.9552 - val_loss: 0.3451 - val_acc: 0.9273\n",
            "Epoch 13/100\n",
            "491/491 [==============================] - 0s 58us/sample - loss: 0.3297 - acc: 0.9572 - val_loss: 0.3190 - val_acc: 0.9273\n",
            "Epoch 14/100\n",
            "491/491 [==============================] - 0s 54us/sample - loss: 0.3022 - acc: 0.9582 - val_loss: 0.2968 - val_acc: 0.9273\n",
            "Epoch 15/100\n",
            "491/491 [==============================] - 0s 59us/sample - loss: 0.2769 - acc: 0.9603 - val_loss: 0.2788 - val_acc: 0.9364\n",
            "Epoch 16/100\n",
            "491/491 [==============================] - 0s 57us/sample - loss: 0.2559 - acc: 0.9613 - val_loss: 0.2613 - val_acc: 0.9364\n",
            "Epoch 17/100\n",
            "491/491 [==============================] - 0s 56us/sample - loss: 0.2361 - acc: 0.9623 - val_loss: 0.2478 - val_acc: 0.9364\n",
            "Epoch 18/100\n",
            "491/491 [==============================] - 0s 64us/sample - loss: 0.2192 - acc: 0.9623 - val_loss: 0.2351 - val_acc: 0.9364\n",
            "Epoch 19/100\n",
            "491/491 [==============================] - 0s 61us/sample - loss: 0.2052 - acc: 0.9644 - val_loss: 0.2236 - val_acc: 0.9364\n",
            "Epoch 20/100\n",
            "491/491 [==============================] - 0s 56us/sample - loss: 0.1919 - acc: 0.9644 - val_loss: 0.2152 - val_acc: 0.9364\n",
            "Epoch 21/100\n",
            "491/491 [==============================] - 0s 56us/sample - loss: 0.1808 - acc: 0.9644 - val_loss: 0.2084 - val_acc: 0.9364\n",
            "Epoch 22/100\n",
            "491/491 [==============================] - 0s 60us/sample - loss: 0.1715 - acc: 0.9654 - val_loss: 0.2023 - val_acc: 0.9364\n",
            "Epoch 23/100\n",
            "491/491 [==============================] - 0s 66us/sample - loss: 0.1628 - acc: 0.9654 - val_loss: 0.1961 - val_acc: 0.9364\n",
            "Epoch 24/100\n",
            "491/491 [==============================] - 0s 66us/sample - loss: 0.1555 - acc: 0.9633 - val_loss: 0.1908 - val_acc: 0.9455\n",
            "Epoch 25/100\n",
            "491/491 [==============================] - 0s 71us/sample - loss: 0.1490 - acc: 0.9654 - val_loss: 0.1889 - val_acc: 0.9455\n",
            "Epoch 26/100\n",
            "491/491 [==============================] - 0s 55us/sample - loss: 0.1433 - acc: 0.9664 - val_loss: 0.1861 - val_acc: 0.9455\n",
            "Epoch 27/100\n",
            "491/491 [==============================] - 0s 56us/sample - loss: 0.1381 - acc: 0.9664 - val_loss: 0.1826 - val_acc: 0.9455\n",
            "Epoch 28/100\n",
            "491/491 [==============================] - 0s 58us/sample - loss: 0.1335 - acc: 0.9664 - val_loss: 0.1800 - val_acc: 0.9455\n",
            "Epoch 29/100\n",
            "491/491 [==============================] - 0s 61us/sample - loss: 0.1294 - acc: 0.9664 - val_loss: 0.1764 - val_acc: 0.9455\n",
            "Epoch 30/100\n",
            "491/491 [==============================] - 0s 52us/sample - loss: 0.1261 - acc: 0.9654 - val_loss: 0.1749 - val_acc: 0.9455\n",
            "Epoch 31/100\n",
            "491/491 [==============================] - 0s 73us/sample - loss: 0.1229 - acc: 0.9664 - val_loss: 0.1754 - val_acc: 0.9455\n",
            "Epoch 32/100\n",
            "491/491 [==============================] - 0s 63us/sample - loss: 0.1194 - acc: 0.9664 - val_loss: 0.1735 - val_acc: 0.9455\n",
            "Epoch 33/100\n",
            "491/491 [==============================] - 0s 61us/sample - loss: 0.1168 - acc: 0.9664 - val_loss: 0.1723 - val_acc: 0.9455\n",
            "Epoch 34/100\n",
            "491/491 [==============================] - 0s 55us/sample - loss: 0.1137 - acc: 0.9664 - val_loss: 0.1699 - val_acc: 0.9455\n",
            "Epoch 35/100\n",
            "491/491 [==============================] - 0s 54us/sample - loss: 0.1113 - acc: 0.9664 - val_loss: 0.1685 - val_acc: 0.9455\n",
            "Epoch 36/100\n",
            "491/491 [==============================] - 0s 57us/sample - loss: 0.1094 - acc: 0.9674 - val_loss: 0.1676 - val_acc: 0.9455\n",
            "Epoch 37/100\n",
            "491/491 [==============================] - 0s 57us/sample - loss: 0.1073 - acc: 0.9674 - val_loss: 0.1636 - val_acc: 0.9455\n",
            "Epoch 38/100\n",
            "491/491 [==============================] - 0s 54us/sample - loss: 0.1062 - acc: 0.9664 - val_loss: 0.1638 - val_acc: 0.9455\n",
            "Epoch 39/100\n",
            "491/491 [==============================] - 0s 67us/sample - loss: 0.1047 - acc: 0.9664 - val_loss: 0.1627 - val_acc: 0.9455\n",
            "Epoch 40/100\n",
            "491/491 [==============================] - 0s 57us/sample - loss: 0.1031 - acc: 0.9674 - val_loss: 0.1661 - val_acc: 0.9455\n",
            "Epoch 41/100\n",
            "491/491 [==============================] - 0s 54us/sample - loss: 0.1021 - acc: 0.9674 - val_loss: 0.1639 - val_acc: 0.9455\n",
            "Epoch 42/100\n",
            "491/491 [==============================] - 0s 57us/sample - loss: 0.1006 - acc: 0.9674 - val_loss: 0.1648 - val_acc: 0.9455\n",
            "Epoch 43/100\n",
            "491/491 [==============================] - 0s 53us/sample - loss: 0.0995 - acc: 0.9684 - val_loss: 0.1644 - val_acc: 0.9455\n",
            "Epoch 44/100\n",
            "491/491 [==============================] - 0s 55us/sample - loss: 0.0992 - acc: 0.9674 - val_loss: 0.1618 - val_acc: 0.9455\n",
            "Epoch 45/100\n",
            "491/491 [==============================] - 0s 57us/sample - loss: 0.0979 - acc: 0.9695 - val_loss: 0.1639 - val_acc: 0.9455\n",
            "Epoch 46/100\n",
            "491/491 [==============================] - 0s 66us/sample - loss: 0.0968 - acc: 0.9695 - val_loss: 0.1627 - val_acc: 0.9455\n",
            "Epoch 47/100\n",
            "491/491 [==============================] - 0s 82us/sample - loss: 0.0962 - acc: 0.9674 - val_loss: 0.1603 - val_acc: 0.9455\n",
            "Epoch 48/100\n",
            "491/491 [==============================] - 0s 57us/sample - loss: 0.0955 - acc: 0.9695 - val_loss: 0.1614 - val_acc: 0.9455\n",
            "Epoch 49/100\n",
            "491/491 [==============================] - 0s 59us/sample - loss: 0.0948 - acc: 0.9695 - val_loss: 0.1627 - val_acc: 0.9455\n",
            "Epoch 50/100\n",
            "491/491 [==============================] - 0s 56us/sample - loss: 0.0938 - acc: 0.9695 - val_loss: 0.1619 - val_acc: 0.9455\n",
            "Epoch 51/100\n",
            "491/491 [==============================] - 0s 64us/sample - loss: 0.0933 - acc: 0.9695 - val_loss: 0.1608 - val_acc: 0.9455\n",
            "Epoch 52/100\n",
            "491/491 [==============================] - 0s 61us/sample - loss: 0.0927 - acc: 0.9695 - val_loss: 0.1599 - val_acc: 0.9455\n",
            "Epoch 53/100\n",
            "491/491 [==============================] - 0s 56us/sample - loss: 0.0922 - acc: 0.9695 - val_loss: 0.1620 - val_acc: 0.9455\n",
            "Epoch 54/100\n",
            "491/491 [==============================] - 0s 56us/sample - loss: 0.0916 - acc: 0.9695 - val_loss: 0.1607 - val_acc: 0.9455\n",
            "Epoch 55/100\n",
            "491/491 [==============================] - 0s 64us/sample - loss: 0.0919 - acc: 0.9695 - val_loss: 0.1633 - val_acc: 0.9455\n",
            "Epoch 56/100\n",
            "491/491 [==============================] - 0s 65us/sample - loss: 0.0912 - acc: 0.9674 - val_loss: 0.1557 - val_acc: 0.9455\n",
            "Epoch 57/100\n",
            "491/491 [==============================] - 0s 54us/sample - loss: 0.0909 - acc: 0.9695 - val_loss: 0.1601 - val_acc: 0.9455\n",
            "Epoch 58/100\n",
            "491/491 [==============================] - 0s 52us/sample - loss: 0.0901 - acc: 0.9695 - val_loss: 0.1585 - val_acc: 0.9455\n",
            "Epoch 59/100\n",
            "491/491 [==============================] - 0s 64us/sample - loss: 0.0898 - acc: 0.9695 - val_loss: 0.1621 - val_acc: 0.9455\n",
            "Epoch 60/100\n",
            "491/491 [==============================] - 0s 55us/sample - loss: 0.0892 - acc: 0.9695 - val_loss: 0.1634 - val_acc: 0.9455\n",
            "Epoch 61/100\n",
            "491/491 [==============================] - 0s 52us/sample - loss: 0.0888 - acc: 0.9695 - val_loss: 0.1622 - val_acc: 0.9455\n",
            "Epoch 62/100\n",
            "491/491 [==============================] - 0s 58us/sample - loss: 0.0884 - acc: 0.9695 - val_loss: 0.1595 - val_acc: 0.9455\n",
            "Epoch 63/100\n",
            "491/491 [==============================] - 0s 59us/sample - loss: 0.0883 - acc: 0.9684 - val_loss: 0.1570 - val_acc: 0.9455\n",
            "Epoch 64/100\n",
            "491/491 [==============================] - 0s 55us/sample - loss: 0.0877 - acc: 0.9684 - val_loss: 0.1581 - val_acc: 0.9455\n",
            "Epoch 65/100\n",
            "491/491 [==============================] - 0s 75us/sample - loss: 0.0873 - acc: 0.9695 - val_loss: 0.1606 - val_acc: 0.9455\n",
            "Epoch 66/100\n",
            "491/491 [==============================] - 0s 60us/sample - loss: 0.0871 - acc: 0.9695 - val_loss: 0.1601 - val_acc: 0.9455\n",
            "Epoch 67/100\n",
            "491/491 [==============================] - 0s 59us/sample - loss: 0.0868 - acc: 0.9695 - val_loss: 0.1603 - val_acc: 0.9455\n",
            "Epoch 68/100\n",
            "491/491 [==============================] - 0s 60us/sample - loss: 0.0869 - acc: 0.9695 - val_loss: 0.1582 - val_acc: 0.9455\n",
            "Epoch 69/100\n",
            "491/491 [==============================] - 0s 58us/sample - loss: 0.0864 - acc: 0.9695 - val_loss: 0.1623 - val_acc: 0.9455\n",
            "Epoch 70/100\n",
            "491/491 [==============================] - 0s 61us/sample - loss: 0.0860 - acc: 0.9695 - val_loss: 0.1606 - val_acc: 0.9455\n",
            "Epoch 71/100\n",
            "491/491 [==============================] - 0s 54us/sample - loss: 0.0859 - acc: 0.9695 - val_loss: 0.1605 - val_acc: 0.9455\n",
            "Epoch 72/100\n",
            "491/491 [==============================] - 0s 75us/sample - loss: 0.0856 - acc: 0.9705 - val_loss: 0.1622 - val_acc: 0.9455\n",
            "Epoch 73/100\n",
            "491/491 [==============================] - 0s 53us/sample - loss: 0.0853 - acc: 0.9705 - val_loss: 0.1592 - val_acc: 0.9455\n",
            "Epoch 74/100\n",
            "491/491 [==============================] - 0s 51us/sample - loss: 0.0851 - acc: 0.9705 - val_loss: 0.1587 - val_acc: 0.9455\n",
            "Epoch 75/100\n",
            "491/491 [==============================] - 0s 55us/sample - loss: 0.0848 - acc: 0.9705 - val_loss: 0.1589 - val_acc: 0.9455\n",
            "Epoch 76/100\n",
            "491/491 [==============================] - 0s 56us/sample - loss: 0.0851 - acc: 0.9705 - val_loss: 0.1612 - val_acc: 0.9455\n",
            "Epoch 77/100\n",
            "491/491 [==============================] - 0s 57us/sample - loss: 0.0845 - acc: 0.9705 - val_loss: 0.1582 - val_acc: 0.9455\n",
            "Epoch 78/100\n",
            "491/491 [==============================] - 0s 57us/sample - loss: 0.0853 - acc: 0.9705 - val_loss: 0.1625 - val_acc: 0.9455\n",
            "Epoch 79/100\n",
            "491/491 [==============================] - 0s 59us/sample - loss: 0.0844 - acc: 0.9715 - val_loss: 0.1564 - val_acc: 0.9455\n",
            "Epoch 80/100\n",
            "491/491 [==============================] - 0s 57us/sample - loss: 0.0841 - acc: 0.9705 - val_loss: 0.1571 - val_acc: 0.9455\n",
            "Epoch 81/100\n",
            "491/491 [==============================] - 0s 67us/sample - loss: 0.0838 - acc: 0.9695 - val_loss: 0.1538 - val_acc: 0.9636\n",
            "Epoch 82/100\n",
            "491/491 [==============================] - 0s 59us/sample - loss: 0.0839 - acc: 0.9684 - val_loss: 0.1551 - val_acc: 0.9545\n",
            "Epoch 83/100\n",
            "491/491 [==============================] - 0s 63us/sample - loss: 0.0835 - acc: 0.9684 - val_loss: 0.1545 - val_acc: 0.9636\n",
            "Epoch 84/100\n",
            "491/491 [==============================] - 0s 62us/sample - loss: 0.0842 - acc: 0.9674 - val_loss: 0.1498 - val_acc: 0.9636\n",
            "Epoch 85/100\n",
            "491/491 [==============================] - 0s 65us/sample - loss: 0.0832 - acc: 0.9664 - val_loss: 0.1534 - val_acc: 0.9636\n",
            "Epoch 86/100\n",
            "491/491 [==============================] - 0s 56us/sample - loss: 0.0829 - acc: 0.9674 - val_loss: 0.1546 - val_acc: 0.9636\n",
            "Epoch 87/100\n",
            "491/491 [==============================] - 0s 54us/sample - loss: 0.0831 - acc: 0.9705 - val_loss: 0.1548 - val_acc: 0.9636\n",
            "Epoch 88/100\n",
            "491/491 [==============================] - 0s 66us/sample - loss: 0.0825 - acc: 0.9725 - val_loss: 0.1588 - val_acc: 0.9455\n",
            "Epoch 89/100\n",
            "491/491 [==============================] - 0s 57us/sample - loss: 0.0832 - acc: 0.9715 - val_loss: 0.1560 - val_acc: 0.9455\n",
            "Epoch 90/100\n",
            "491/491 [==============================] - 0s 64us/sample - loss: 0.0824 - acc: 0.9715 - val_loss: 0.1564 - val_acc: 0.9455\n",
            "Epoch 91/100\n",
            "491/491 [==============================] - 0s 63us/sample - loss: 0.0822 - acc: 0.9715 - val_loss: 0.1540 - val_acc: 0.9636\n",
            "Epoch 92/100\n",
            "491/491 [==============================] - 0s 56us/sample - loss: 0.0823 - acc: 0.9705 - val_loss: 0.1548 - val_acc: 0.9455\n",
            "Epoch 93/100\n",
            "491/491 [==============================] - 0s 60us/sample - loss: 0.0833 - acc: 0.9684 - val_loss: 0.1505 - val_acc: 0.9636\n",
            "Epoch 94/100\n",
            "491/491 [==============================] - 0s 62us/sample - loss: 0.0823 - acc: 0.9715 - val_loss: 0.1571 - val_acc: 0.9455\n",
            "Epoch 95/100\n",
            "491/491 [==============================] - 0s 65us/sample - loss: 0.0818 - acc: 0.9715 - val_loss: 0.1550 - val_acc: 0.9455\n",
            "Epoch 96/100\n",
            "491/491 [==============================] - 0s 71us/sample - loss: 0.0815 - acc: 0.9705 - val_loss: 0.1530 - val_acc: 0.9636\n",
            "Epoch 97/100\n",
            "491/491 [==============================] - 0s 60us/sample - loss: 0.0816 - acc: 0.9705 - val_loss: 0.1539 - val_acc: 0.9636\n",
            "Epoch 98/100\n",
            "491/491 [==============================] - 0s 55us/sample - loss: 0.0815 - acc: 0.9705 - val_loss: 0.1523 - val_acc: 0.9636\n",
            "Epoch 99/100\n",
            "491/491 [==============================] - 0s 65us/sample - loss: 0.0812 - acc: 0.9705 - val_loss: 0.1532 - val_acc: 0.9636\n",
            "Epoch 100/100\n",
            "491/491 [==============================] - 0s 56us/sample - loss: 0.0811 - acc: 0.9705 - val_loss: 0.1504 - val_acc: 0.9636\n",
            "Finished training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLNR0zUP-W00",
        "colab_type": "text"
      },
      "source": [
        "Now let's plot the loss based on the training epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jlm4HU2A31mj",
        "colab_type": "code",
        "outputId": "860113a3-1ad1-450d-c647-4d3bca53a5e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel(\"Loss Magnitude\")\n",
        "plt.plot(history.history['loss'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f418a032198>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhIN4WCiIh0+/8rIN1FUVSfUQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCFMfk-_36KY",
        "colab_type": "code",
        "outputId": "a740efb2-f028-49c0-9416-6a92b7eebdaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "eval_model=model.evaluate(X_train, y_train)\n",
        "eval_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "546/546 [==============================] - 0s 43us/sample - loss: 0.0879 - acc: 0.9689\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.08786988179216455, 0.96886444]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s9UaORt-fkR",
        "colab_type": "text"
      },
      "source": [
        "The training accuracy is 96.8%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXEWGBSO4ZM1",
        "colab_type": "code",
        "outputId": "3670177a-a252-4c02-d188-2cfee42089fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from numpy import argmax\n",
        "y_pred_probs = model.predict(X_test)\n",
        "#inverted = argmax(y_pred)\n",
        "y_pred = [argmax(y) for y in y_pred_probs]\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "inverted = [argmax(y) for y in y_test]\n",
        "cm = confusion_matrix(inverted, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix\")\n",
        "print(cm)\n",
        "print(\"Accuracy is: {0:.2f}%\".format(100*(cm[0,0] + cm[1,1])/sum(sum(cm))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "[[80  1]\n",
            " [ 2 54]]\n",
            "Accuracy is: 97.81%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtX4L3qW-pmu",
        "colab_type": "text"
      },
      "source": [
        "The accuracy in the unseen test set is 97.81% with only 1 False Positive and 2 False Negatives."
      ]
    }
  ]
}